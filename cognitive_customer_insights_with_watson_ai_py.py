# -*- coding: utf-8 -*-
"""Cognitive Customer Insights with Watson AI.py

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1fcajFw7QhaHjMq0cUcoeRtzLxmXE0oor
"""

!pip install pandas numpy matplotlib seaborn scikit-learn ibm-watson-machine-learning flask-ngrok

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder, StandardScaler
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score, classification_report
from ibm_watson_machine_learning import APIClient
import pickle

# Load the dataset from the uploaded file
file_path = "/content/WA_Fn-UseC_-Telco-Customer-Churn.csv"  # Update path if needed
df = pd.read_csv(file_path)

# Display the first few rows
print(df.head())

# Check basic info
df.info()

# Check for missing values
print(df.isnull().sum())

# Summary statistics
print(df.describe())

# Fill missing numerical values with the mean
numerical_cols = df.select_dtypes(include=np.number).columns
df[numerical_cols] = df[numerical_cols].fillna(df[numerical_cols].mean())

# Fill missing categorical values with the mode
categorical_cols = df.select_dtypes(exclude=np.number).columns
df[categorical_cols] = df[categorical_cols].fillna(df[categorical_cols].mode().iloc[0])

# Check for missing values
print(df.isnull().sum())

# Fill missing values with mean for numerical data only
for column in df.select_dtypes(include=['number']).columns: #Select only columns with numerical data types
    df[column].fillna(df[column].mean(), inplace=True) #Fill NA values for the selected column with the column's mean

label_encoders = {}
categorical_cols = df.select_dtypes(include=["object"]).columns

for col in categorical_cols:
    le = LabelEncoder()
    df[col] = le.fit_transform(df[col])
    label_encoders[col] = le  # Save encoders for later use

scaler = StandardScaler()
numerical_cols = df.select_dtypes(include=["int64", "float64"]).columns
df[numerical_cols] = scaler.fit_transform(df[numerical_cols])

X = df.drop(columns=["Churn"])  # Drop target column
y = df["Churn"]  # Define target variable

# Split dataset
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Initialize and train the model
# Use RandomForestRegressor for continuous target variables
from sklearn.ensemble import RandomForestRegressor #Import RandomForestRegressor
model = RandomForestRegressor(n_estimators=100, random_state=42) # Change to RandomForestRegressor
model.fit(X_train, y_train)

# Save model locally for later deployment
pickle.dump(model, open("churn_model.pkl", "wb"))

# Predict on test data
y_pred = model.predict(X_test)

# Calculate R-squared (a common regression metric)
from sklearn.metrics import r2_score #Import r2_score
r2 = r2_score(y_test, y_pred)
print(f"R-squared: {r2:.4f}")

# You can also consider other regression metrics like Mean Squared Error (MSE) or Mean Absolute Error (MAE)
from sklearn.metrics import mean_squared_error, mean_absolute_error #Import MSE and MAE
mse = mean_squared_error(y_test, y_pred)
mae = mean_absolute_error(y_test, y_pred)
print(f"Mean Squared Error: {mse:.4f}")
print(f"Mean Absolute Error: {mae:.4f}")

wml_credentials = {
    "apikey": "fajcYKSClkw1E8nKFjbwyoKAPBZ5ZLyX6rpCsfQNOTrW",
    "url": "https://us-south.ml.cloud.ibm.com"
}

# Connect to WML
client = APIClient(wml_credentials)

# Set Deployment Space
client.set.default_space("19983994-8987-465b-90d8-40e2562538bb")  # Replace with your IBM Cloud Deployment Space ID

# Fetch the correct software specification UID
software_spec_uid = client.software_specifications.get_id_by_name("runtime-22.1-py3.10")
print("Software Specification UID:", software_spec_uid)

client.software_specifications.get_id_by_name("runtime-22.1-py3.10")

client.software_specifications.list()

software_spec_uid = client.software_specifications.get_id_by_name("runtime-22.1-py3.10")
print(software_spec_uid)

software_spec_uid = client.software_specifications.get_id_by_name("runtime-24.1-py3.11")
print("Software Specification UID:", software_spec_uid)

model_metadata = {
    client.repository.ModelMetaNames.NAME: "CustomerChurnDeployment",
    client.repository.ModelMetaNames.TYPE: "scikit-learn_1.0",
    client.repository.ModelMetaNames.SOFTWARE_SPEC_UID: software_spec_uid
}

import shutil

# Create a zip file containing the model
shutil.make_archive("/content/churn_model", 'zip', "/content", "churn_model.pkl")

import os
import shutil
import json

# Define the required directory for Watsonx
model_dir = "/content/model"
os.makedirs(model_dir, exist_ok=True)  # Create the 'model' folder if it doesn't exist

# Copy the model file into the 'model' directory
shutil.copy("/content/churn_model.pkl", model_dir)  # Ensure 'churn_model.pkl' is in /content/

# Define the metadata required for Watsonx
metadata = {
    "name": "Customer Churn Model",
    "description": "A machine learning model to predict customer churn.",
    "type": "scikit-learn_1.0",  # Make sure this matches your model framework
    "runtime": "python-3.10"
}

# Save metadata as JSON inside the 'model' folder
metadata_path = os.path.join(model_dir, "model_metadata.json")
with open(metadata_path, "w") as f:
    json.dump(metadata, f)

# Create a .zip file with the correct structure
shutil.make_archive("/content/CustomerChurnModel", 'zip', "/content", "model")

print("Zip file 'CustomerChurnModel.zip' created successfully!")

import zipfile

# List contents of the zip file
with zipfile.ZipFile("/content/CustomerChurnModel.zip", 'r') as zip_ref:
    zip_ref.printdir()

from ibm_watson_machine_learning import APIClient

# IBM Cloud Credentials (Replace with your actual API key)
wml_credentials = {
    "apikey": "G7N6hMqTBR5BO0o0YjwgkbblwAiWW38GnsMUsmRihIKV",  # Use a new secure API key
    "url": "https://us-south.ml.cloud.ibm.com"
}

# Connect to IBM Watson ML
client = APIClient(wml_credentials)

# Set Deployment Space (Replace with your actual Space GUID)
space_id = "cce3afdc-7a71-45a7-a9f6-3d51184bbd59"  # Your Deployment Space GUID
client.set.default_space(space_id)

print("Successfully connected to IBM Watson Machine Learning!")

# Fetch all models in the deployment space
models = client.repository.get_details()

# Print full response to check the structure
import json
print(json.dumps(models, indent=4))  # Pretty-print the API response

# Fetch model details
models = client.repository.get_details()

# Ensure 'resources' key exists before accessing it
if "resources" in models and models["resources"]:
    model_id = None
    model_name = "Customer Churn Model"  # Ensure name matches exactly

    for model in models["resources"]:
        if model["metadata"]["name"] == model_name:
            model_id = model["metadata"]["id"]
            break

    if model_id:
        print(" Model ID:", model_id)
    else:
        print(" Model not found in the deployment space.")
else:
    print(" No models found. Check if models exist in IBM Cloud.")

wml_credentials = {
    "apikey": "6X4Ku6ke_nnHpZldiy6nqEkyTVlS42c_-rH_HSJ5GgSz",
    "url": "https://us-south.ml.cloud.ibm.com"
}

from ibm_watson_machine_learning import APIClient

try:
    # Authenticate with IBM Cloud
    client = APIClient(wml_credentials)
    print(" Successfully connected to IBM Watson Machine Learning!")
except Exception as e:
    print(" Authentication failed. Error:", str(e))

from ibm_watson_machine_learning import APIClient

# IBM Cloud API Key (Replace with your actual key)
wml_credentials = {
    "apikey": "6X4Ku6ke_nnHpZldiy6nqEkyTVlS42c_-rH_HSJ5GgSz",
    "url": "https://us-south.ml.cloud.ibm.com"
}

# Connect to IBM Watson ML
client = APIClient(wml_credentials)

# Set Deployment Space
space_id = "cce3afdc-7a71-45a7-a9f6-3d51184bbd59"
client.set.default_space(space_id)

# Fetch Deployment ID
deployments = client.deployments.get_details()
deployment_id = None

for deployment in deployments.get('resources', []):
    if deployment["metadata"]["name"] == "Customer Churn Deployment":
        deployment_id = deployment["metadata"]["id"]
        break

if deployment_id:
    print("Deployment ID:", deployment_id)
else:
    print("Deployment not found. Ensure itâ€™s created correctly.")

import requests

# IBM Cloud API Key (Replace with your actual API key)
api_key = "6X4Ku6ke_nnHpZldiy6nqEkyTVlS42c_-rH_HSJ5GgSz"

# IAM Authentication URL
auth_url = "https://iam.cloud.ibm.com/identity/token"

# Request IAM Token
auth_response = requests.post(
    auth_url,
    data={"grant_type": "urn:ibm:params:oauth:grant-type:apikey", "apikey": api_key},
    headers={"Content-Type": "application/x-www-form-urlencoded"},
)

# Extract IAM Token
iam_token = auth_response.json()["access_token"]
print("IAM Token generated successfully!")

# Define API Endpoint (Use the public endpoint from Watsonx)
deployment_id = "7fc085b2-6164-42c7-a36a-c8060be63ddb"
deployment_endpoint = f"https://us-south.ml.cloud.ibm.com/ml/v4/deployments/{deployment_id}/predictions?version=2021-05-01"

# Define Headers with Updated IAM Token
headers = {
    "Authorization": f"Bearer {iam_token}",
    "Content-Type": "application/json"
}

# Example Input Data (Modify fields as per your model input)
payload = {
    "input_data": [
        {
            "fields": ["SeniorCitizen", "tenure", "MonthlyCharges", "TotalCharges"],  # Adjust based on model input
            "values": [[0, 12, 79.85, 1290.10]]  # Example test case
        }
    ]
}

# Send API Request
response = requests.post(deployment_endpoint, json=payload, headers=headers)

# Print Prediction Response
print(" Prediction Response:", response.json())

{
  "predictions": [
    {
      "fields": ["prediction", "confidence"],
      "values": [["No", 0.85]]
    }
  ]
}

!pip install flask flask-ngrok pyngrok requests

!pip install flask flask-cors
from flask import Flask, request, jsonify
from flask_cors import CORS

app = Flask(__name__)
CORS(app)  # Enable CORS for cross-origin requests

@app.route('/')
def home():
    return " Welcome to the Customer Churn Prediction API! Use /predict endpoint."

@app.route('/predict', methods=['POST'])
def predict():
    try:
        data = request.get_json()
        if not data:
            return jsonify({"error": "Invalid request. No data received."}), 400

        response = {
            "predictions": [
                {
                    "fields": ["prediction", "confidence"],
                    "values": [["No", 0.85]]
                }
            ]
        }
        return jsonify(response)

    except Exception as e:
        return jsonify({"error": str(e)}), 500

# âœ… Start Flask Server
from google.colab.output import eval_js
print("ðŸš€ Flask API is starting... Click the link below when ready.")

def get_public_url(port=5000):
    return eval_js(f"google.colab.kernel.proxyPort({port})")

app.run(host='0.0.0.0', port=5000)